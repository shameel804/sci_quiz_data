[
    {
        "title": "Big O Notation â±ï¸",
        "ques": "Explain **Big O notation** and rank these: O(1), O(n), O(nÂ²), O(log n), O(n log n).",
        "answer": {
            "type": "text",
            "content": "**Big O Notation:**\nDescribes algorithm efficiency as input size grows (worst-case)\n\n**Ranking (best to worst):**\n\n| Complexity | Name | Example |\n|------------|------|--------|\n| **O(1)** | Constant | Array access |\n| **O(log n)** | Logarithmic | Binary search |\n| **O(n)** | Linear | Simple loop |\n| **O(n log n)** | Linearithmic | Merge sort |\n| **O(nÂ²)** | Quadratic | Nested loops |\n\n**Growth comparison (n=1000):**\nO(1)=1, O(log n)â‰ˆ10, O(n)=1000, O(n log n)â‰ˆ10000, O(nÂ²)=1,000,000"
        },
        "explanation": "Big O ignores constants and lower-order terms. Focus on dominant behavior as nâ†’âˆ."
    },
    {
        "title": "Sorting Algorithms ğŸ”€",
        "ques": "Compare **Bubble Sort, Merge Sort, and Quick Sort** in terms of time complexity.",
        "answer": {
            "type": "text",
            "content": "**Sorting Algorithm Comparison:**\n\n| Algorithm | Best | Average | Worst | Space |\n|-----------|------|---------|-------|-------|\n| **Bubble Sort** | O(n) | O(nÂ²) | O(nÂ²) | O(1) |\n| **Merge Sort** | O(n log n) | O(n log n) | O(n log n) | O(n) |\n| **Quick Sort** | O(n log n) | O(n log n) | O(nÂ²) | O(log n) |\n\n**Key Points:**\n- Bubble: Simple but slow\n- Merge: Stable, consistent, uses extra memory\n- Quick: Fast average case, unstable"
        },
        "explanation": "In practice, Quick Sort is often fastest due to cache efficiency. Merge Sort preferred when stability matters."
    },
    {
        "title": "Binary Search ğŸ”",
        "ques": "Write pseudocode for **binary search** and explain why it's O(log n).",
        "answer": {
            "type": "code",
            "content": "function binarySearch(arr, target):\n    left = 0\n    right = length(arr) - 1\n    \n    while left <= right:\n        mid = (left + right) / 2\n        \n        if arr[mid] == target:\n            return mid  // Found!\n        else if arr[mid] < target:\n            left = mid + 1  // Search right half\n        else:\n            right = mid - 1  // Search left half\n    \n    return -1  // Not found",
            "lang": "python"
        },
        "explanation": "Each comparison eliminates half the remaining elements. For n elements: need logâ‚‚(n) comparisons. Requires sorted array."
    },
    {
        "title": "Recursion ğŸ”„",
        "ques": "Explain **recursion** with the Fibonacci sequence example. What's the problem with naive recursion?",
        "answer": {
            "type": "text",
            "content": "**Recursion:** Function calls itself with simpler input\n\n**Fibonacci (naive):**\n```\nfib(n):\n    if n <= 1: return n\n    return fib(n-1) + fib(n-2)\n```\n\n**Problem:** Exponential time O(2â¿)\n- fib(5) calls fib(4) and fib(3)\n- Both call overlapping subproblems\n- fib(2) computed 3 times!\n\n**Solution: Memoization**\nStore results to avoid recalculation â†’ O(n)"
        },
        "explanation": "Every recursive solution can be converted to iteration. Key: base case (stopping condition) and progress toward it."
    },
    {
        "title": "Graph Traversal ğŸ•¸ï¸",
        "ques": "Compare **BFS** (Breadth-First Search) and **DFS** (Depth-First Search).",
        "answer": {
            "type": "text",
            "content": "**BFS (Breadth-First Search):**\n- Explores level by level\n- Uses **queue** (FIFO)\n- Finds shortest path (unweighted)\n- Good for: Nearest neighbor, shortest path\n\n**DFS (Depth-First Search):**\n- Explores as deep as possible first\n- Uses **stack** (LIFO) or recursion\n- May not find shortest path\n- Good for: Topological sort, maze solving\n\n**Time/Space:** Both O(V + E) time, O(V) space"
        },
        "explanation": "BFS guarantees shortest path in unweighted graphs. DFS uses less memory for long paths."
    },
    {
        "title": "Dynamic Programming ğŸ“Š",
        "ques": "Explain **dynamic programming** with the knapsack problem.",
        "answer": {
            "type": "text",
            "content": "**Dynamic Programming:**\nBreak problem into overlapping subproblems, solve each once, store results\n\n**0/1 Knapsack Problem:**\nGiven items with weights and values, maximize value in capacity W\n\n**Recurrence:**\n```\ndp[i][w] = max(\n    dp[i-1][w],           // Don't take item i\n    dp[i-1][w-wt[i]] + v[i]  // Take item i\n)\n```\n\n**Time:** O(n Ã— W)\n**Space:** O(n Ã— W), can be optimized to O(W)"
        },
        "explanation": "DP requirements: optimal substructure + overlapping subproblems. Bottom-up (tabulation) vs top-down (memoization)."
    },
    {
        "title": "Hash Tables âš¡",
        "ques": "How do **hash tables** achieve O(1) average lookup? What are collisions?",
        "answer": {
            "type": "text",
            "content": "**Hash Table:**\nArray + hash function for key-value storage\n\n**How it works:**\n1. Key â†’ hash function â†’ index\n2. Store value at that index\n3. Retrieval: same process\n\n**Collision:** Two keys hash to same index\n\n**Handling Collisions:**\n| Method | Description |\n|--------|------------|\n| **Chaining** | Linked list at each bucket |\n| **Open addressing** | Probe for next empty slot |\n\n**Load factor = n/buckets** â†’ resize when too high"
        },
        "explanation": "Average O(1), worst O(n) if many collisions. Good hash functions distribute keys uniformly."
    },
    {
        "title": "Divide and Conquer âœ‚ï¸",
        "ques": "Explain the **divide and conquer** paradigm with Merge Sort.",
        "answer": {
            "type": "text",
            "content": "**Divide and Conquer Steps:**\n1. **Divide:** Split problem into subproblems\n2. **Conquer:** Solve subproblems recursively\n3. **Combine:** Merge solutions\n\n**Merge Sort:**\n```\nmergeSort(arr):\n    if len(arr) <= 1: return arr\n    \n    mid = len(arr) / 2\n    left = mergeSort(arr[0:mid])    // Divide\n    right = mergeSort(arr[mid:])    // Divide\n    return merge(left, right)        // Combine\n```\n\n**Analysis:** T(n) = 2T(n/2) + O(n) = O(n log n)"
        },
        "explanation": "Other D&C algorithms: Binary search, Quick sort, Strassen's matrix multiplication, FFT."
    },
    {
        "title": "Greedy Algorithms ğŸ¤‘",
        "ques": "What is a **greedy algorithm**? When does it give optimal solutions?",
        "answer": {
            "type": "text",
            "content": "**Greedy Algorithm:**\nAlways make locally optimal choice at each step\n\n**Example: Coin Change (US coins)**\nFor 63Â¢: Take largest coin possible each time\n- 50Â¢ â†’ 10Â¢ â†’ 1Â¢ â†’ 1Â¢ â†’ 1Â¢ = 5 coins âœ“\n\n**When Greedy Works:**\n- **Greedy choice property:** Local optimal leads to global optimal\n- **Optimal substructure:** Optimal solution contains optimal solutions to subproblems\n\n**Greedy Fails:**\nCoins: [1, 15, 25], Amount: 30\n- Greedy: 25+1+1+1+1+1 = 6 coins\n- Optimal: 15+15 = 2 coins"
        },
        "explanation": "Greedy is simpler and faster than DP but doesn't always work. Must prove correctness for each problem."
    },
    {
        "title": "NP-Complete Problems ğŸ”",
        "ques": "Explain **P vs NP** and what it means for a problem to be NP-complete.",
        "answer": {
            "type": "text",
            "content": "**Complexity Classes:**\n\n| Class | Definition | Example |\n|-------|------------|--------|\n| **P** | Solvable in polynomial time | Sorting |\n| **NP** | Verifiable in polynomial time | Sudoku |\n| **NP-Complete** | Hardest in NP | Traveling Salesman |\n| **NP-Hard** | At least as hard as NP-complete | Halting problem |\n\n**P = NP?**\nBillion-dollar question! If true, any problem we can verify quickly, we can solve quickly.\n\n**Implications:** If one NP-complete solved in P, all are."
        },
        "explanation": "Most believe P â‰  NP. For NP-complete problems, we use approximation algorithms or heuristics."
    }
]