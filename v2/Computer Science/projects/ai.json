[
    {
        "title": "Machine Learning Types ü§ñ",
        "ques": "Compare **supervised, unsupervised, and reinforcement learning**.",
        "answer": {
            "type": "text",
            "content": "| Type | Data | Goal | Examples |\n|------|------|------|----------|\n| **Supervised** | Labeled | Predict labels | Classification, Regression |\n| **Unsupervised** | Unlabeled | Find patterns | Clustering, Dimensionality reduction |\n| **Reinforcement** | Rewards/penalties | Maximize reward | Game playing, Robotics |\n\n**Examples:**\n- Supervised: Spam detection, House price prediction\n- Unsupervised: Customer segmentation, Anomaly detection\n- Reinforcement: AlphaGo, Self-driving cars"
        },
        "explanation": "Most business applications use supervised learning. Deep learning is a subset using neural networks."
    },
    {
        "title": "Neural Networks üß†",
        "ques": "Explain the basic structure of a **neural network** and how it learns.",
        "answer": {
            "type": "text",
            "content": "**Structure:**\n- **Input layer:** Receives data\n- **Hidden layers:** Process features\n- **Output layer:** Produces prediction\n\n**Neurons:**\n- Receive inputs with weights\n- Sum weighted inputs + bias\n- Apply activation function\n- Pass output forward\n\n**Learning (Backpropagation):**\n1. Forward pass: Compute prediction\n2. Calculate loss (error)\n3. Backward pass: Compute gradients\n4. Update weights to reduce loss\n\n**Epoch:** One pass through all training data"
        },
        "explanation": "Deep learning = many hidden layers. Common activations: ReLU, sigmoid, softmax."
    },
    {
        "title": "Natural Language Processing üìù",
        "ques": "What are key **NLP** tasks and techniques?",
        "answer": {
            "type": "text",
            "content": "**NLP Tasks:**\n| Task | Example |\n|------|--------|\n| **Tokenization** | Split text into words |\n| **Named Entity Recognition** | Find names, places |\n| **Sentiment Analysis** | Positive/negative classification |\n| **Machine Translation** | English ‚Üí French |\n| **Text Generation** | ChatGPT |\n\n**Key Techniques:**\n- Word embeddings (Word2Vec, GloVe)\n- RNNs/LSTMs for sequences\n- Transformers (BERT, GPT)\n- Attention mechanisms"
        },
        "explanation": "Transformers revolutionized NLP in 2017. Large Language Models (LLMs) like GPT-4 use billions of parameters."
    },
    {
        "title": "Computer Vision üëÅÔ∏è",
        "ques": "Describe common **computer vision** tasks and how CNNs work.",
        "answer": {
            "type": "text",
            "content": "**Computer Vision Tasks:**\n- Image classification\n- Object detection\n- Segmentation\n- Face recognition\n- Image generation\n\n**CNN (Convolutional Neural Network):**\n| Layer | Purpose |\n|-------|--------|\n| **Convolutional** | Detect features with filters |\n| **Pooling** | Reduce dimensions |\n| **Fully Connected** | Classification |\n\n**How Convolution Works:**\n- Slide filter over image\n- Compute dot product at each position\n- Creates feature map"
        },
        "explanation": "Early layers detect edges, later layers detect complex features. Transfer learning uses pre-trained CNNs."
    },
    {
        "title": "Search Algorithms üîé",
        "ques": "Explain **A* search** algorithm and how it differs from Dijkstra's.",
        "answer": {
            "type": "text",
            "content": "**A* Search:**\nFinds shortest path using:\n$$f(n) = g(n) + h(n)$$\n\n| Component | Meaning |\n|-----------|--------|\n| g(n) | Cost from start to n |\n| h(n) | Heuristic estimate to goal |\n| f(n) | Total estimated cost |\n\n**vs Dijkstra's:**\n- Dijkstra's: h(n) = 0 (no heuristic)\n- A*: Uses heuristic for faster search\n\n**Heuristics:**\n- Manhattan distance (grid)\n- Euclidean distance\n- Must be admissible (never overestimate)"
        },
        "explanation": "A* is optimal if heuristic is admissible. Used in GPS navigation, game pathfinding."
    },
    {
        "title": "Generative AI üé®",
        "ques": "How do **generative AI models** like GPT and DALL-E work?",
        "answer": {
            "type": "text",
            "content": "**Large Language Models (GPT):**\n- Trained on massive text data\n- Predict next token given context\n- Transformer architecture\n- Fine-tuned with RLHF (Reinforcement Learning from Human Feedback)\n\n**Image Generation (DALL-E, Stable Diffusion):**\n- Diffusion models\n- Start with noise, gradually denoise\n- Conditioned on text prompt\n- Learn to reverse noise process\n\n**Key Concepts:**\n- Attention mechanism\n- Self-supervision\n- Emergent capabilities"
        },
        "explanation": "Generative AI creates new content. Raises questions about copyright, misinformation, and AI safety."
    },
    {
        "title": "Decision Trees üå≥",
        "ques": "How do **decision trees** and **random forests** work?",
        "answer": {
            "type": "text",
            "content": "**Decision Tree:**\n- Hierarchical \"if-then\" rules\n- Split data on features\n- Use entropy/Gini for best splits\n- Prone to overfitting\n\n**Random Forest:**\n- Ensemble of many decision trees\n- Each tree trained on random subset\n- Features randomly selected\n- Vote/average for prediction\n\n**Advantages of Random Forest:**\n- More accurate than single tree\n- Less overfitting\n- Handles missing data\n- Feature importance"
        },
        "explanation": "Ensemble methods combine weak learners into strong ones. Gradient Boosting (XGBoost) is another powerful ensemble."
    },
    {
        "title": "Bias and Fairness ‚öñÔ∏è",
        "ques": "What causes **bias in AI systems** and how can it be mitigated?",
        "answer": {
            "type": "text",
            "content": "**Sources of Bias:**\n| Source | Example |\n|--------|--------|\n| Training data | Historical discrimination |\n| Feature selection | Proxies for protected attributes |\n| Sampling | Underrepresented groups |\n| Labeling | Subjective judgments |\n\n**Mitigation Strategies:**\n- Diverse training data\n- Bias testing and auditing\n- Fairness constraints in models\n- Human oversight\n- Explainable AI (XAI)\n\n**Fairness Definitions:**\nDemographic parity, equal opportunity, individual fairness"
        },
        "explanation": "AI ethics is critical. COMPAS recidivism algorithm showed significant racial bias."
    },
    {
        "title": "Turing Test ü§ñ",
        "ques": "What is the **Turing Test** and why is it important in AI?",
        "answer": {
            "type": "text",
            "content": "**Turing Test (1950):**\n- Proposed by Alan Turing\n- Can a machine exhibit intelligent behavior indistinguishable from a human?\n\n**Setup:**\n- Human evaluator converses with machine and human\n- Can they tell which is which?\n- If not, machine passes the test\n\n**Criticisms:**\n- Focuses on deception, not intelligence\n- Chinese Room argument (Searle)\n- Modern LLMs can pass but may not \"understand\"\n\n**Alternatives:** Winograd Schema Challenge, ARC benchmark"
        },
        "explanation": "The Turing Test sparked debate about machine intelligence. Understanding vs. simulation remains an open question."
    },
    {
        "title": "AI Safety üõ°Ô∏è",
        "ques": "What are the major concerns in **AI safety** research?",
        "answer": {
            "type": "text",
            "content": "**AI Safety Concerns:**\n\n| Issue | Description |\n|-------|------------|\n| **Alignment** | Ensuring AI goals match human values |\n| **Robustness** | Handling edge cases, adversarial attacks |\n| **Interpretability** | Understanding AI decisions |\n| **Control** | Maintaining human oversight |\n| **Misuse** | Deepfakes, autonomous weapons |\n\n**Alignment Problem:**\n- Specification: Defining correct objectives\n- Generalization: Working in new situations\n- Corrigibility: Allowing correction"
        },
        "explanation": "AI safety research aims to ensure AI systems remain beneficial. Leading organizations: DeepMind, Anthropic, OpenAI."
    }
]